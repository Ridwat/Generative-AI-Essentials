{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Assignment  \n",
        "## Text Generation Using LSTM Neural Networks  \n",
        "**Student:** Adetola Ridwat Odulaja  \n",
        "**Course:** Data Analytics & Artificial Intelligence  \n",
        "**Date:** November 2025\n"
      ],
      "metadata": {
        "id": "1IRg0zYAyXkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction  \n",
        "Generative Artificial Intelligence (Generative AI) refers to a class of machine learning techniques that enable machines to produce new content such as text, images, audio, and code. These systems learn patterns from large datasets and generate creative, human-like outputs.  \n",
        "In recent years, transformer-based models such as GPT (Generative Pre-trained Transformers) have revolutionized language generation by using attention mechanisms and large-scale training.\n",
        "\n",
        "This assignment explores the fundamentals of Generative AI, focusing on its significance, architecture, use cases, and ethical considerations. It also includes a practical implementation of a text generation model trained on a public domain dataset.\n"
      ],
      "metadata": {
        "id": "Kf3GZZ60ybrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Architecture and Functionality  \n",
        "\n",
        "GPT models are built on the Transformer Decoder architecture. Their key components include:\n",
        "\n",
        "### • Self-Attention  \n",
        "Allows the model to determine which words in a sentence are most relevant when predicting the next token.\n",
        "\n",
        "### • Multi-Head Attention  \n",
        "Multiple attention heads run in parallel to capture different linguistic relationships such as grammar and meaning.\n",
        "\n",
        "### • Positional Encoding  \n",
        "Provides the model with information about the order of words since transformers do not process data sequentially by default.\n",
        "\n",
        "### • Feedforward Neural Networks  \n",
        "Applies non-linear transformations to improve pattern recognition.\n",
        "\n",
        "### • Autoregressive Text Generation  \n",
        "GPT generates text one token at a time using probability distributions learned during training.\n",
        "\n",
        "Together, these mechanisms allow GPT models to generate fluent, contextual, and human-like text.\n"
      ],
      "metadata": {
        "id": "uqULYCYeyekf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "For this project, I used \"Alice in Wonderland\" by Lewis Carroll, obtained from Project Gutenberg (public domain text).  \n",
        "The text was cleaned, converted to lowercase, tokenized, and transformed into input sequences for training the model.\n",
        "\n",
        "A word-level Tokenizer was used to reduce memory usage and ensure the training process runs efficiently within Google Colab.\n"
      ],
      "metadata": {
        "id": "X466_nxpyhb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_name = next(iter(uploaded))\n",
        "text = open(file_name, 'r', encoding='utf-8').read().lower()\n",
        "print(\"Loaded:\", file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "ca30irlAtr0b",
        "outputId": "f8534d5e-8bc1-4abe-c4c0-b75745fad68c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15f00f6b-e960-4638-aaa5-6abeea8ef566\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-15f00f6b-e960-4638-aaa5-6abeea8ef566\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving alice.txt.crdownload to alice.txt (1).crdownload\n",
            "Loaded: alice.txt (1).crdownload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean and Tokenize\n"
      ],
      "metadata": {
        "id": "zJkRDsVEtxQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Clean text\n",
        "text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1\n",
        "\n",
        "print(\"Total words:\", total_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6-kDB-StvRf",
        "outputId": "41cd1bec-2a50-40e9-aa51-bbd8eeac8573"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 3031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "words = text.split()\n",
        "\n",
        "seq_length = 20  # small size for RAM efficiency\n",
        "\n",
        "for i in range(seq_length, len(words)):\n",
        "    seq = words[i-seq_length:i+1]\n",
        "    encoded = tokenizer.texts_to_sequences([' '.join(seq)])[0]\n",
        "    input_sequences.append(encoded)\n",
        "\n",
        "input_sequences = np.array(input_sequences)\n",
        "print(\"Total sequences:\", input_sequences.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqbrYuHfvhGv",
        "outputId": "e52f296a-e045-46e6-ad43-0e3ae5ae2d20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: (30559, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
      ],
      "metadata": {
        "id": "fuBN5tT-vkxr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 128, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(total_words, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "mj30q_-Wvpc9",
        "outputId": "daf39075-9a93-401f-9d23-ba69fd15fcb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=5, batch_size=128, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_OFti4rvr-a",
        "outputId": "4b6dc789-c996-4e27-caec-3fc8f12927c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 87ms/step - accuracy: 0.0455 - loss: 6.7801\n",
            "Epoch 2/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 97ms/step - accuracy: 0.0607 - loss: 5.9615\n",
            "Epoch 3/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 88ms/step - accuracy: 0.0738 - loss: 5.7716\n",
            "Epoch 4/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 88ms/step - accuracy: 0.0929 - loss: 5.5827\n",
            "Epoch 5/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 87ms/step - accuracy: 0.1137 - loss: 5.3867\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d4c73f12f60>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words=30):\n",
        "    for _ in range(next_words):\n",
        "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "\n",
        "        pred = model.predict(encoded, verbose=0)\n",
        "        next_word = tokenizer.index_word[np.argmax(pred)]\n",
        "        seed_text += \" \" + next_word\n",
        "    return seed_text\n",
        "\n",
        "print(generate_text(\"alice looked around\", 40))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Wut-x5wcDd",
        "outputId": "13b4b521-1afa-43f0-c30e-bfde1ed03e06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice looked around the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the queen and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results  \n",
        "\n",
        "The LSTM model was trained for 5 epochs with a sequence length of 20 words.  \n",
        "During training, accuracy improved steadily from approximately **4%** to **11%**, showing that the model successfully learned basic next-word prediction patterns.\n",
        "\n",
        "A sample generated output from the seed text “alice looked around” produced repetitive but grammatically connected text. This behavior is typical of small LSTM models, especially when trained on limited data without advanced attention mechanisms.\n",
        "\n",
        "Overall, the model demonstrated the core principles of generative modeling—learning distributions and predicting the next word based on context.\n"
      ],
      "metadata": {
        "id": "gK6n69SxynUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ethical Considerations in Generative AI  \n",
        "\n",
        "Generative AI raises several important ethical issues, including:\n",
        "\n",
        "### • Bias  \n",
        "Models may learn harmful stereotypes if trained on biased data.\n",
        "\n",
        "### • Misinformation  \n",
        "AI systems may generate information that appears real but is factually incorrect.\n",
        "\n",
        "### • Privacy  \n",
        "Models trained on unfiltered datasets may unintentionally reproduce private or sensitive information.\n",
        "\n",
        "### • Copyright Concerns  \n",
        "Using copyrighted training data without permissions can cause legal and ethical risks.\n",
        "\n",
        "### • Responsible Use  \n",
        "Generative AI should not be deployed in ways that cause harm, deceive users, or manipulate public opinion.\n",
        "\n",
        "Mitigating these risks requires transparent data sourcing, bias audits, human monitoring, and responsible deployment practices.\n"
      ],
      "metadata": {
        "id": "YdVpuf6RyrN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion  \n",
        "\n",
        "In this assignment, I explored the foundations of Generative AI, focusing on transformer-based models like GPT and how they generate human-like text. The hands-on component involved training a lightweight LSTM text generation model on the public domain text “Alice in Wonderland,” covering the end-to-end workflow from data cleaning and tokenization to model training and inference.\n",
        "\n",
        "### Summary of Results  \n",
        "The model successfully learned basic language patterns and demonstrated next-word prediction capabilities. Training accuracy improved from approximately 4% to 11%, and the model generated continuous text given a starting prompt. Although the output contained repetition and lacked deeper coherence compared to advanced GPT models, it illustrated the essential mechanisms behind generative modeling.\n",
        "\n",
        "This project also highlighted key ethical considerations related to bias, misinformation, privacy, and responsible AI deployment.  \n",
        "\n",
        "Overall, this assignment deepened my understanding of both theoretical and practical aspects of Generative AI. Future work may include experimenting with transformer-based architectures, fine-tuning pretrained GPT models, or exploring multimodal generative systems that combine text with images, audio, and other data types.\n"
      ],
      "metadata": {
        "id": "lSS5bBoUys1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References  \n",
        "• Project Gutenberg – https://www.gutenberg.org  \n",
        "• TensorFlow Documentation – https://www.tensorflow.org  \n",
        "• Keras Tokenizer API – https://keras.io/api/preprocessing/text  \n",
        "• “Attention Is All You Need” (Vaswani et al., 2017)\n"
      ],
      "metadata": {
        "id": "QlnBBttPyxM6"
      }
    }
  ]
}