# ğŸ“˜ Generative AI Text Generation Project
*A practical implementation of a lightweight generative model using LSTM neural networks*

---

## ğŸ“Œ Project Overview
This project demonstrates the fundamentals of **Generative Artificial Intelligence (Generative AI)** by implementing a simple text generation model using **TensorFlow/Keras**.  
The model is trained on the public-domain text *Alice in Wonderland* from Project Gutenberg and uses an **LSTM (Long Short-Term Memory)** network to generate new text sequences based on a seed input.

This project helps illustrate:
- How generative models learn linguistic patterns  
- How next-word prediction works  
- The difference between small LSTM models and GPT-style transformers  
- Ethical considerations in AI  

---

## ğŸ“Œ Key Features
- âœ” Word-level tokenization using Keras Tokenizer  
- âœ” Sequence generation from raw text  
- âœ” LSTM-based generative neural network  
- âœ” Next-word prediction text generation  
- âœ” Visualization of training accuracy/loss  
- âœ” Ethical considerations of Generative AI  
- âœ” Exportable notebook for PDF submission  

---

## ğŸ“ Dataset
**Alice in Wonderland â€“ Lewis Carroll (Plain Text UTF-8)**  
Source: https://www.gutenberg.org

This text is in the public domain and legally free to use.

---

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ README.md
â”œâ”€â”€ generative_ai_text_generation.ipynb   # Main Google Colab Notebook
â”œâ”€â”€ alice.txt                             # Dataset (optional, upload in Colab)
â””â”€â”€ report.pdf                            # Exported PDF report (optional)
```

---

## ğŸ§  Model Architecture
The implemented model uses:
- **Embedding Layer** â€“ Converts tokens into word vectors  
- **LSTM Layer (128 units)** â€“ Learns sequential text patterns  
- **Dense Softmax Output Layer** â€“ Predicts the next word  

This lightweight architecture is designed to run efficiently within free Google Colab RAM limits.

---

## ğŸš€ How to Run the Project

### 1. Upload the Dataset
Download â€œAlice in Wonderland (Plain Text UTF-8)â€ and upload it in Colab:
```python
from google.colab import files
uploaded = files.upload()
```

### 2. Import Libraries
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

### 3. Train the Model
```python
model.fit(X, y, epochs=5, batch_size=128)
```
Training accuracy rises from ~4% to ~11%.

### 4. Generate Text
```python
print(generate_text("alice looked around", 40))
```

---

## ğŸ“Š Results Summary
- The model successfully learned basic linguistic patterns.  
- Accuracy improved across epochs (approximately 4% â†’ 11%).  
- Generated text showed simple sequencing patterns typical of small LSTMs.  
- Demonstrated the foundational principle of next-word prediction.  
- Highlights contrast between small models vs. large GPT architectures.

---

## âš–ï¸ Ethical Considerations
The project discusses the following:
- **Bias** in training data  
- **Privacy risks** and potential data leakage  
- **Misinformation** generated by AI systems  
- **Copyright concerns** when using protected datasets  
- **Responsible deployment** practices  

These considerations are essential when building and deploying generative AI tools.

---

## ğŸ“„ Report
A full academic-style report is included inside the Colab notebook and can be exported as PDF.  
The report includes:
- Introduction  
- GPT Architecture  
- Methodology  
- Model Implementation  
- Results  
- Applications  
- Ethical Considerations  
- Conclusion  

---

## ğŸ‘©â€ğŸ’» Author
**Adetola Ridwat Odulaja**  
Student â€“ Data Analytics & Artificial Intelligence  

---

## ğŸ“œ License
This project uses only public-domain text and open-source libraries.

---
